# iOS 渲染原理解析


#### 1、渲染与成像

##### 1.1、`CPU+GPU` 渲染

对于现代计算机系统，简单来说可以大概视作三层架构：硬件、操作系统与进程。对于移动端来说，进程就是 App，而 CPU 与 GPU 是硬件层面的重要组成部分。CPU 与 GPU 提供了计算能力，通过操作系统被 App 调用。

* CPU（Central Processing Unit）：中央处理器，系统的运算核心、控制核心；所做的工作大都在软件层面；
* GPU（Graphics Processing Unit）：图形处理器，所做的工作大都在硬件层面；其中可进行绘图运算工作的专用微处理器，是连接计算机和显示终端的纽带。

__区别__： CUP 和 GPU 之所以大不相同，是由于其设计目的的不同，它们分别针对了两种不同的应用场景：
* CPU 是运算核心与控制核心， 需要很强的通用性来处理各种不同的类型数据，同时又要逻辑判断又会引入大量的分支跳转和中断处理。这些都使得CPU的内部结构异常复杂。
* GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断纯净的计算环境。


![CPU与GPU.png](https://upload-images.jianshu.io/upload_images/7112462-a43a7562c29fb9b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


因此，CPU 与 GPU 的架构也不同。因为 CPU 面临的情况更加复杂，因此从上图中也可以看出，CPU 拥有更多的缓存空间 Cache 以及复杂的控制单元，计算能力并不是 CPU 的主要诉求。CPU 是设计目标是低时延，更多的高速缓存也意味着可以更快地访问数据；同时复杂的控制单元也能更快速地处理逻辑分支，更适合串行计算。

而 GPU 基于大吞吐量而设计，拥有更多的计算单元 Arithmetic Logic Unit，具有更强的计算能力。
GPU 优秀的并行计算能力使其能够快速将图形结果计算出来并在屏幕的所有像素中进行显示。

###### 1.1.1、图像渲染流水线

图像渲染流程粗粒度地大概分为下面这些步骤：

![图像渲染流水线.png](https://upload-images.jianshu.io/upload_images/7112462-748b7eaa6a870b87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


上述图像渲染流水线中，除了第一部分 Application 阶段，后续主要都由 GPU 负责，为了方便后文讲解，先将 GPU 的渲染流程图展示出来：

![GPU渲染流程图.png](https://upload-images.jianshu.io/upload_images/7112462-cad7d01d3dc2824f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


上图就是一个三角形被渲染的过程中，GPU 所负责的渲染流水线。可以看到简单的三角形绘制就需要大量的计算，如果再有更多更复杂的顶点、颜色、纹理信息（包括 3D 纹理），那么计算量是难以想象的。这也是为什么 GPU 更适合于渲染流程。

接下来，具体讲解渲染流水线中各个部分的具体任务：

###### 1.2.2、Application 应用处理阶段：得到图元

这个阶段具体指的就是图像在应用中被处理的阶段，此时还处于 CPU 负责的时期。在这个阶段应用可能会对图像进行一系列的操作或者改变，最终将新的图像信息传给下一阶段。这部分信息被叫做图元，通常是三角形、线段、顶点等。


###### 1.2.3、Geometry 几何处理阶段：处理图元


这个阶段以及之后的阶段，主要由 GPU 负责。此时 GPU 可以拿到上一个阶段传递下来的图元信息、并对这部分图元进行处理，然后输出新的图元。这一系列阶段包括：
* 顶点着色器 `Vertex Shader`：该阶段将图元中的顶点信息进行视角转换、添加光照信息、增加纹理等操作；
* 形状装配 `Shape Assembly`：图元中的三角形、线段、点分别对应三个 顶点、两个 顶点、一个 顶点。这个阶段会将`顶点`连接成相对应的形状。
* 几何着色器`Geometry Shader`：添加额外的顶点，将原始图元转换成新图元，以构建一个不一样的模型。简单来说就是基于三角形、线段和点构建更复杂的几何图形。

###### 1.2.4、Rasterization 光栅化阶段：图元转换为像素

光栅化的主要目的是将几何渲染之后的图元信息，转换为一系列的像素，以便后续显示在屏幕上。这个阶段中会根据图元信息，计算出每个图元所覆盖的像素信息等，从而将像素划分成不同的部分。

![光栅化.png](https://upload-images.jianshu.io/upload_images/7112462-fe7aed6152423776.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


一种简单的划分就是根据中心点，如果像素的中心点在图元内部，那么这个像素就属于这个图元。如上图所示，深蓝色的线就是图元信息所构建出的三角形；而通过是否覆盖中心点，可以遍历出所有属于该图元的所有像素，即浅蓝色部分。


###### 1.2.5、Pixel 像素处理阶段：处理像素，得到位图

经过上述光栅化阶段，我们得到了图元所对应的像素，此时，我们需要给这些像素填充颜色和效果。所以最后这个阶段就是给像素填充正确的内容，最终显示在屏幕上。这些经过处理、蕴含大量信息的像素点集合，被称作位图`bitmap`。也就是说，Pixel 阶段最终输出的结果就是位图，过程具体包含：


* 片段着色器`Fragment Shader`：也叫做 `Pixel Shader`，这个阶段的目的是给每一个像素 Pixel 赋予正确的颜色。颜色的来源就是之前得到的顶点、纹理、光照等信息。由于需要处理纹理、光照等复杂信息，所以这通常是 _整个系统的性能瓶颈_。
* 测试与混合`Tests and Blending`：也叫做 `Merging 阶段` ，这个阶段主要处理片段的前后位置以及透明度。这个阶段会检测各个着色片段的深度值 `z` 坐标，从而判断片段的前后位置，以及是否应该被舍弃。同时也会计算相应的透明度 `alpha` 值，从而进行片段的混合，得到最终的颜色。


这些点可以进行不同的排列和染色以构成图样。当放大位图时，可以看见赖以构成整个图像的无数单个方块。只要有足够多的不同色彩的像素，就可以制作出色彩丰富的图象，逼真地表现自然界的景象。缩放和旋转容易失真，同时文件容量较大。


##### 1.2、屏幕成像

在图像渲染流程结束之后，接下来就需要将得到的像素信息显示在物理屏幕上了。GPU 最后一步渲染结束之后像素信息，被存在帧缓冲区`Framebuffer`中，之后显示控制器`VideoController`会读取`Framebuffer`中的信息，经过数模转换传递给显示器`Monitor`，进行显示。完整的流程如下图所示：

![屏幕成像流程.png](https://upload-images.jianshu.io/upload_images/7112462-f5debf28b322ce1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


上图所示为常见的 CPU、GPU、显示器工作方式。CPU 计算好显示内容提交至 GPU，GPU 渲染完成后将渲染结果（位图）存入帧缓冲区，显示控制器会按照 VSync 信号逐帧读取帧缓冲区的数据，经过数据转换后最终由显示器进行显示。

##### 1.3、显示器原理

显示器原理：显示器 的电子束从上到下逐行扫描，扫描完成后显示器就呈现一帧画面；然后电子束回到初始位置进行下一次扫描。
为了同步显示器的显示过程和系统的显示控制器，显示器会用硬件时钟产生一系列的定时信号：
* 水平同步信号`horizonal synchronization`，简称 `HSync`：当电子束换行进行扫描时，显示器会发出一个`HSync`信号；
* 垂直同步信号`vertical synchronization`，简称`VSync`： 当一帧画面绘制完成后，电子束回复到原位，准备画下一帧前，显示器会发出一个 `VSync` 信号；

![电子束扫描.png](https://upload-images.jianshu.io/upload_images/7112462-8902ca4c6d3a37d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



电子束扫描的过程中，屏幕就能呈现出对应的结果，每次整个屏幕被电子束扫描完一次后，就相当于呈现了一帧完整的图像。屏幕不断地刷新，不停呈现新的帧，就能呈现出连续的影像。而这个屏幕刷新的频率，就是帧率（Frame per Second，FPS）。由于人眼的视觉暂留效应，当屏幕刷新频率足够高时（FPS 通常是 50 到 60 左右），就能让画面看起来是连续而流畅的。_对于 iOS 而言，App 应该尽量保证 60 FPS 才是最好的体验_。

##### 1.4、屏幕撕裂 Screen Tearing

在这种单一缓存的模式下，最理想的情况就是一个流畅的流水线：每次电子束从头开始新的一帧的扫描时，CPU+GPU 对于该帧的渲染流程已经结束，渲染好的位图已经放入`Framebuffer`中。但这种完美的情况是非常脆弱的，很容易产生屏幕撕裂：

![屏幕撕裂.jpeg](https://upload-images.jianshu.io/upload_images/7112462-2224adb1148a3144.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


CPU+GPU 的渲染流程是一个非常耗时的过程。如果在电子束开始扫描新的一帧时，位图还没有渲染好，而是在扫描到屏幕中间时才渲染完成，被放入`Framebuffer`中； 那么已扫描的部分就是上一帧的画面，而未扫描的部分则会显示新的一帧图像，这就造成屏幕撕裂。

##### 1.5、垂直同步 Vsync + 双缓冲机制 Double Buffering

> iOS 设备会始终使用 Vsync + Double Buffering 的策略。

解决屏幕撕裂、提高显示效率的一个策略就是使用垂直同步信号 Vsync 与双缓冲机制 Double Buffering。

使用 `Vsync`信号给 `Framebuffer` 加锁：只有当显示控制器接收到 `Vsync`信号 之后，才会将`Framebuffer`中的位图更新为下一帧，这样就能保证每次显示的都是同一帧的画面，因而避免了屏幕撕裂。

这种情况下要求：显示控制器在接受到 `Vsync` 信号 之后将下一帧的位图传入；这意味着整个  CPU+GPU 的渲染流程都要在一瞬间完成，这是明显不现实的。
所以使用双缓冲机制会增加一个新的_备用缓冲区_`BackBuffer`：渲染结果会预先保存在 `BackBuffer` 中；在接收到 `Vsync` 信号的时候，显示控制器会将 `BackBuffer` 中的内容置换到  `Framebuffer`  中，此时就能保证置换操作几乎在一瞬间完成（实际上是交换了内存地址）。


![双缓冲.png](https://upload-images.jianshu.io/upload_images/7112462-33bb699421bd12e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



##### 1.6、屏幕卡顿的本质：掉帧 Jank

> 屏幕刷新频率必须要足够高才能流畅


使用  `Vsync ` 信号以及双缓冲机制之后，能够解决屏幕撕裂的问题，但是会引入新的问题：掉帧。
如果在接收到  `Vsync` 之时 CPU 和 GPU 还没有渲染好新的位图，显示控制器就不会去替换`Framebuffer`  中的位图；这时屏幕就会重新扫描呈现出上一帧一模一样的画面。相当于两个周期显示了同样的画面，这就是所谓掉帧的情况。

![掉帧.png](https://upload-images.jianshu.io/upload_images/7112462-693e98154ec1bc41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如图所示，A、B 代表两个`Framebuffer`，当 B 没有渲染完毕时就接收到了 `Vsync` 信号，所以屏幕只能再显示相同帧 A，这就发生了第一次的掉帧。


App 卡顿的直接原因：CPU 和 GPU 渲染流水线耗时过长，导致掉帧。对于 iPhone 手机来说，屏幕最大的刷新频率是 60 FPS，一般只要保证 50 FPS 就已经是较好的体验了。但是如果掉帧过多，导致刷新频率过低，就会造成不流畅的使用体验。


##### 1.7、三缓冲 Triple Buffering

在上述策略中发生掉帧的时候，CPU 和 GPU 有一段时间处于闲置状态：当 A 的内容正在被扫描显示在屏幕上，而 B 的内容已经被渲染好，此时 CPU 和 GPU 就处于闲置状态。
如果再增加一个帧缓冲区，就可以利用这段时间进行下一步的渲染，并将渲染结果暂存于新增的帧缓冲区。

![三缓冲.png](https://upload-images.jianshu.io/upload_images/7112462-daf73e687a662d52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如图所示，由于增加了新的帧缓冲区，可以一定程度上地利用掉帧的空档期，合理利用 CPU 和 GPU 性能，从而减少掉帧的次数。

在Android4.1系统开始，引入了三缓冲+垂直同步的机制。由于多加了一个 Buffer，实现了 CPU 跟 GPU 并行，便可以做到了只在开始掉一帧，后续却不掉帧。

Vsync 与双缓冲的意义：强制同步屏幕刷新，以掉帧为代价解决屏幕撕裂问题。
三缓冲的意义：合理使用 CPU、GPU 渲染性能，减少掉帧次数。



#### 2、 iOS 中的渲染框架

iOS App 的图形渲染依然符合渲染流水线的基本架构；在硬件基础之上，使用了 `Core Graphics`、`Core Animation`、`Core Image` 等框架来绘制可视化内容，这些软件框架相互之间也有着依赖关系。这些框架都需要通过 `OpenGL` 来调用 GPU 进行绘制，最终将内容显示到屏幕之上。


![iOS渲染框架.png](https://upload-images.jianshu.io/upload_images/7112462-90a5841fcf762bc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




* `GPU Driver`：上述软件框架相互之间也有着依赖关系，不过所有框架最终都会通过 `OpenGL` 连接到 `GPU Driver`，`GPU Driver` 是直接和 GPU 交流的代码块，直接与 GPU 连接。

* `OpenGL`：是一个提供了 2D 和 3D 图形渲染的 API，它能和 GPU 密切的配合，最高效地利用 GPU 的能力，实现硬件加速渲染。`OpenGL`的高效实现（利用了图形加速硬件）一般由显示设备厂商提供，而且非常依赖于该厂商提供的硬件。`OpenGL` 之上扩展出很多东西，如 `Core Graphics` 等最终都依赖于 `OpenGL`，有些情况下为了更高的效率，比如游戏程序，甚至会直接调用 `OpenGL` 的接口。

* `Core Animation`：是一个复合引擎，其职责是尽可能快地组合屏幕上不同的可视内容，这些可视内容可被分解成独立的图层`CALayer`，这些图层会被存储在一个叫做图层树的体系之中。在 iOS 上，几乎所有的东西都是通过 `Core Animation` 绘制出来，它的自由度更高，使用范围也更广。

* `Core Graphics`：基于 `Quartz` 高级绘图引擎，主要用于 _运行时绘制图像_，在运行时实时计算、绘制一系列图像帧来实现动画 ；是一个强大的二维图像绘制引擎，用来处理基于路径的绘图，转换，颜色管理，离屏渲染，图案，渐变和阴影，图像数据管理，图像创建和图像遮罩以及 PDF 文档创建，显示和分析。常用的 `CGRect` 就定义在这个框架下。

* `Core Image`： 一个高性能的图像处理分析的框架，它拥有一系列现成的图像滤镜，能对已存在的图像进行高效的处理。

* `Metal`： 类似于 `OpenGL ES`，也是一套第三方标准，具体实现由苹果实现。`Core Animation`、`Core Image`、`SceneKit`、`SpriteKit` 等等渲染框架都是构建于 `Metal` 之上的。


##### 2.1、Core Animation 渲染流水线

> App 本身并不负责渲染，渲染由一个独立的进程 `Render Server`负责。


App 通过 IPC（_进程间通信_） 将渲染任务及相关数据提交给 `Render Server`。`Render Server` 处理完数据后，再传递至 GPU。最后由 GPU 调用 iOS 的图像设备进行显示。

![Core Animation 渲染流水线.png](https://upload-images.jianshu.io/upload_images/7112462-9a9612f016a3417b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


* 首先，由 App 处理 `Handle Events`事件，如：用户的点击操作；在此过程中 App 可能需要更新 视图树，相应地，图层树也会被更新。
* 其次，App 通过 CPU 完成对显示内容的计算，如：视图的创建、布局计算、图片解码、文本绘制等。在完成对显示内容的计算之后，App 对图层进行打包，并在下一次 `RunLoop` 时将其发送至 `Render Server`，即完成了一次 `Commit Transaction` 操作。
* `Render Server` 主要执行 `Open GL`、`Core Graphics` 相关程序，并调用 GPU。
* GPU 则在物理层上完成了对图像的渲染。
* 最终，GPU 通过 `FrameBuffer`、显示控制器等相关部件，将图像显示在屏幕上。



Core Animation，它本质上可以理解为一个复合引擎，主要职责包含：渲染、构建和实现动画。

通常我们会使用 Core Animation 来高效、方便地实现动画，但是实际上它的前身叫做 Layer Kit，关于动画实现只是它功能中的一部分。对于 iOS app，不论是否直接使用了 Core Animation，它都在底层深度参与了 app 的构建。而对于 OS X app，也可以通过使用 Core Animation 方便地实现部分功能。

![框架层.png](https://upload-images.jianshu.io/upload_images/7112462-e55b192bb303bd9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


Core Animation 是 AppKit 和 UIKit 完美的底层支持，同时也被整合进入 Cocoa 和 Cocoa Touch 的工作流之中，它是 app 界面渲染和构建的最基础架构。Core Animation 的职责就是尽可能快地组合屏幕上不同的可视内容，这个内容是被分解成独立的 layer（iOS 中具体而言就是 CALayer），并且被存储为树状层级结构。这个树也形成了 UIKit 以及在 iOS 应用程序当中你所能在屏幕上看见的一切的基础。

简单来说就是用户能看到的屏幕上的内容都由 CALayer 进行管理。那么 CALayer 究竟是如何进行管理的呢？另外在 iOS 开发过程中，最大量使用的视图控件实际上是 UIView 而不是 CALayer，那么他们两者的关系到底如何呢？



##### 3.2、CALayer 是显示的基础：存储 bitmap

简单理解，CALayer 就是屏幕显示的基础。那 CALayer 是如何完成的呢？让我们来从源码向下探索一下，在 CALayer.h 中，CALayer 有这样一个属性 contents：


```
/** Layer content properties and methods. **/
 
/* An object providing the contents of the layer, typically a CGImageRef,
 * but may be something else. (For example, NSImage objects are
 * supported on Mac OS X 10.6 and later.) Default value is nil.
 * Animatable. */
 
@property(nullable, strong) id contents;
```

contents 提供了 layer 的内容，是一个指针类型，在 iOS 中的类型就是 CGImageRef（在 OS X 中还可以是 NSImage）。而我们进一步查到，Apple 对 CGImageRef 的定义是：

> A bitmap image or image mask.

看到 bitmap，这下我们就可以和之前讲的的渲染流水线联系起来了：实际上，CALayer 中的 contents 属性保存了由设备渲染流水线渲染好的位图 bitmap（通常也被称为 backing store），而当设备屏幕进行刷新时，会从 CALayer 中读取生成好的 bitmap，进而呈现到屏幕上。

所以，如果我们在代码中对 CALayer 的 contents 属性进行了设置，比如这样：

```
// 注意 CGImage 和 CGImageRef 的关系：
// typedef struct CGImage CGImageRef;
layer.contents = (__bridge id)image.CGImage;**
```

那么在运行时，操作系统会调用底层的接口，将 image 通过 CPU+GPU 的渲染流水线渲染得到对应的 bitmap，存储于 CALayer.contents 中，在设备屏幕进行刷新的时候就会读取 bitmap 在屏幕上呈现。

也正因为每次要被渲染的内容是被静态的存储起来的，所以每次渲染时，Core Animation 会触发调用 drawRect: 方法，使用存储好的 bitmap 进行新一轮的展示。


##### 3.3、CALayer 与 UIView 的关系

UIView 作为最常用的视图控件，和 CALayer 也有着千丝万缕的联系，那么两者之间到底是个什么关系，他们有什么差异？

当然，两者有很多显性的区别，比如是否能够响应点击事件。但为了从根本上彻底搞懂这些问题，我们必须要先搞清楚两者的职责。


根据 Apple 的官方文档，UIView 是 app 中的基本组成结构，定义了一些统一的规范。它会负责内容的渲染以及，处理交互事件。具体而言，它负责的事情可以归为下面三类

Drawing and animation：绘制与动画

Layout and subview management：布局与子 view 的管理

Event handling：点击事件处理



而从 CALayer 的官方文档中我们可以看出，CALayer 的主要职责是管理内部的可视内容，这也和我们前文所讲的内容吻合。当我们创建一个 UIView 的时候，UIView 会自动创建一个 CALayer，为自身提供存储 bitmap 的地方（也就是前文说的 backing store），并将自身固定设置为 CALayer 的代理。

![UIView 与 CALayer.png](https://upload-images.jianshu.io/upload_images/7112462-3fa740201d9f21ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




#### 3、离屏渲染 Offscreen Rendering 

##### 3.1、什么是离屏渲染？

正常的渲染流程如下图所示：

![正常渲染流程.png](https://upload-images.jianshu.io/upload_images/7112462-02c0dc5e52ef9315.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


App 通过 CPU 与 GPU 的合作，不停地将内容渲染完成放入 `Framebuffer` 中，而屏幕不断地从 `Framebuffer` 中获取内容，显示实时的内容。如果有时因为面临一些限制，无法把渲染结果直接写入`Framebuffer`，而是先暂存在另外的内存区域，之后再写入`Framebuffer`，那么这个过程被称之为_离屏渲染_。

![离屏渲染流程.png](https://upload-images.jianshu.io/upload_images/7112462-fbcfaff19593195e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


_离屏渲染_需要先额外创建离屏渲染缓冲区 `Offscreen Buffer`，将提前渲染好的内容放入其中，等到合适的时机再将 `Offscreen Buffer` 中的内容进一步叠加、渲染，完成后将结果切换到 Framebuffer 中。

##### 3.2、 CPU _离屏渲染_

>  通过CPU渲染就是俗称的 _软件渲染_，而真正的离屏渲染发生在GPU。

如果在 `UIView` 中实现了 `-drawRect:` 方法，就算它的函数体内部实际没有代码，系统也会为这个`view`申请一块内存区域，等待`CoreGraphics`可能的绘画操作。

对于类似这种 _新开一块 `CGContext` 来画图_ 的操作，也称之为 _CPU离屏渲染_（因为像素数据是暂时存入了`CGContext`，而不是直接到了`Framebuffer`）。进一步来说，其实所有CPU进行的光栅化操作如文字渲染、图片解码等，都无法直接绘制到由GPU掌管的`Framebuffer` ，只能暂时先放在另一块内存之中，说起来都属于“离屏渲染”。

自然我们会认为，因为CPU不擅长做这件事，所以我们需要尽量避免它，就误以为这就是需要避免离屏渲染的原因。但是根据苹果工程师的说法，CPU渲染并非真正意义上的离屏渲染。另一个证据是，如果你的view实现了drawRect，此时打开Xcode调试的“Color offscreen rendered yellow”开关，你会发现这片区域不会被标记为黄色，说明Xcode并不认为这属于离屏渲染。



##### 5.2、离屏渲染的效率问题

从上面的流程来看，离屏渲染时由于 App 需要提前对部分内容进行额外的渲染并保存到 Offscreen Buffer，以及需要在必要时刻对 Offscreen Buffer 和 Framebuffer 进行内容切换，所以会需要更长的处理时间（实际上这两步关于 buffer 的切换代价都非常大）。

并且 Offscreen Buffer 本身就需要额外的空间，大量的离屏渲染可能早能内存的过大压力。与此同时，Offscreen Buffer 的总大小也有限，不能超过屏幕总像素的 2.5 倍。

可见离屏渲染的开销非常大，一旦需要离屏渲染的内容过多，很容易造成掉帧的问题。所以大部分情况下，我们都应该尽量避免离屏渲染。


##### 5.3、为什么使用离屏渲染

那么为什么要使用离屏渲染呢？主要是因为下面这两种原因：

一些特殊效果需要使用额外的 Offscreen Buffer 来保存渲染的中间状态，所以不得不使用离屏渲染。

处于效率目的，可以将内容提前渲染保存在 Offscreen Buffer 中，达到复用的目的。

对于第一种情况，也就是不得不使用离屏渲染的情况，一般都是系统自动触发的，比如阴影、圆角等等。




##### 5.4、光栅化

开启光栅化后，会触发离屏渲染，Render Server 会强制将 CALayer 的渲染位图结果 bitmap 保存下来，这样下次再需要渲染时就可以直接复用，从而提高效率。

而保存的 bitmap 包含 layer 的 subLayer、圆角、阴影、组透明度 group opacity 等，所以如果 layer 的构成包含上述几种元素，结构复杂且需要反复利用，那么就可以考虑打开光栅化。

圆角、阴影、组透明度等会由系统自动触发离屏渲染，那么打开光栅化可以节约第二次及以后的渲染时间。而多层 subLayer 的情况由于不会自动触发离屏渲染，所以相比之下会多花费第一次离屏渲染的时间，但是可以节约后续的重复渲染的开销。

不过使用光栅化的时候需要注意以下几点：

如果 layer 不能被复用，则没有必要打开光栅化

如果 layer 不是静态，需要被频繁修改，比如处于动画之中，那么开启离屏渲染反而影响效率

离屏渲染缓存内容有时间限制，缓存内容 100ms 内如果没有被使用，那么就会被丢弃，无法进行复用

离屏渲染缓存空间有限，超过 2.5 倍屏幕像素大小的话也会失效，无法复用


##### 5.5、圆角的离屏渲染

通常来讲，设置了 layer 的圆角效果之后，会自动触发离屏渲染。但是究竟什么情况下设置圆角才会触发离屏渲染呢？


![圆角的离屏渲染 .png](https://upload-images.jianshu.io/upload_images/7112462-3900fb35275978cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


##### 5.6、离屏渲染的具体逻辑

刚才说了圆角加上 masksToBounds 的时候，因为 masksToBounds 会对 layer 上的所有内容进行裁剪，从而诱发了离屏渲染，那么这个过程具体是怎么回事呢，下面我们来仔细讲一下。

图层的叠加绘制大概遵循“画家算法”，在这种算法下会按层绘制，首先绘制距离较远的场景，然后用绘制距离较近的场景覆盖较远的部分。


在普通的 layer 绘制中，上层的 sublayer 会覆盖下层的 sublayer，下层 sublayer 绘制完之后就可以抛弃了，从而节约空间提高效率。所有 sublayer 依次绘制完毕之后，整个绘制过程完成，就可以进行后续的呈现了。假设我们需要绘制一个三层的 sublayer，不设置裁剪和圆角，那么整个绘制过程就如下图所示：


而当我们设置了 cornerRadius 以及 masksToBounds 进行圆角 + 裁剪时，如前文所述，masksToBounds 裁剪属性会应用到所有的 sublayer 上。这也就意味着所有的 sublayer 必须要重新被应用一次圆角+裁剪，这也就意味着所有的 sublayer 在第一次被绘制完之后，并不能立刻被丢弃，而必须要被保存在 Offscreen buffer 中等待下一轮圆角+裁剪，这也就诱发了离屏渲染，具体过程如下：



实际上不只是圆角+裁剪，如果设置了透明度+组透明（layer.allowsGroupOpacity+layer.opacity），阴影属性（shadowOffset 等）都会产生类似的效果，因为组透明度、阴影都是和裁剪类似的，会作用与 layer 以及其所有 sublayer 上，这就导致必然会引起离屏渲染。



##### 5.7、避免圆角离屏渲染

由于刚才我们提到，圆角引起离屏渲染的本质是裁剪的叠加，导致 masksToBounds 对 layer 以及所有 sublayer 进行二次处理。那么我们只要避免使用 masksToBounds 进行二次处理，而是对所有的 sublayer 进行预处理，就可以只进行“画家算法”，用一次叠加就完成绘制。

那么可行的实现方法大概有下面几种：

【换资源】直接使用带圆角的图片，或者替换背景色为带圆角的纯色背景图，从而避免使用圆角裁剪。不过这种方法需要依赖具体情况，并不通用。

【mask】再增加一个和背景色相同的遮罩 mask 覆盖在最上层，盖住四个角，营造出圆角的形状。但这种方式难以解决背景色为图片或渐变色的情况。

【UIBezierPath】用贝塞尔曲线绘制闭合带圆角的矩形，在上下文中设置只有内部可见，再将不带圆角的 layer 渲染成图片，添加到贝塞尔矩形中。这种方法效率更高，但是 layer 的布局一旦改变，贝塞尔曲线都需要手动地重新绘制，所以需要对 frame、color 等进行手动地监听并重绘。

【CoreGraphics】重写 drawRect:，用 CoreGraphics 相关方法，在需要应用圆角时进行手动绘制。不过 CoreGraphics 效率也很有限，如果需要多次调用也会有效率问题。


##### 5.8、触发离屏渲染原因的总结

总结一下，下面几种情况会触发离屏渲染：

使用了 mask 的 layer (layer.mask)

需要进行裁剪的 layer (layer.masksToBounds / view.clipsToBounds)

设置了组透明度为 YES，并且透明度不为 1 的 layer (layer.allowsGroupOpacity/layer.opacity)

添加了投影的 layer (layer.shadow*)

采用了光栅化的 layer (layer.shouldRasterize)

绘制了文字的 layer (UILabel, CATextLayer, Core Text 等)

不过，需要注意的是，重写 drawRect: 方法并不会触发离屏渲染。前文中我们提到过，重写 drawRect: 会将 GPU 中的渲染操作转移到 CPU 中完成，并且需要额外开辟内存空间。但根据苹果工程师的说法，这和标准意义上的离屏渲染并不一样，在 Instrument 中开启 Color offscreen rendered yellow 调试时也会发现这并不会被判断为离屏渲染。



----

参考文章

[iOS 渲染原理解析](https://blog.csdn.net/Desgard_Duan/article/details/106394306)
[关于iOS离屏渲染的深入研究](https://zhuanlan.zhihu.com/p/72653360)
[iOS下的图像渲染原理](https://juejin.cn/post/6847009772730843149)
